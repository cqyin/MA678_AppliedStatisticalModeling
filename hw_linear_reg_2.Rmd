---
title: "Homework 02"
author: "Chaoqun Yin"
date: "Septemeber 16, 2018"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

\newcommand{\mat}[1]{\boldsymbol{#1}} 
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\rv}[1]{\underline{#1}}

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,dev="CairoPNG",fig.align = "center", 
                      fig.width = 5.656, fig.height = 4, global.par = TRUE)
pacman::p_load("arm","data.table","Cairo","faraway","foreign","ggplot2","knitr")
par (mar=c(3,3,2,1), mgp=c(2,.7,0), tck=-.01)
```

# Introduction 
In homework 2 you will fit many regression models.  You are welcome to explore beyond what the question is asking you.  

Please come see us we are here to help.

## Data analysis 

### Analysis of earnings and height data

The folder `earnings` has data from the Work, Family, and Well-Being Survey (Ross, 1990).
You can find the codebook at http://www.stat.columbia.edu/~gelman/arm/examples/earnings/wfwcodebook.txt
```{r}
gelman_dir <- "http://www.stat.columbia.edu/~gelman/arm/examples/"
heights    <- read.dta (paste0(gelman_dir,"earnings/heights.dta"))
```

Pull out the data on earnings, sex, height, and weight.

1. In R, check the dataset and clean any unusually coded data.

```{r}
head(complete.cases(heights)) #check the missing data
heights.complete <- na.omit(heights) #omit missing data

head(heights.complete == 0) #check the 0 data
zerodata <- which(heights.complete == 0)#omit 0
heights.complete <- heights.complete[-zerodata,]
```

2. Fit a linear regression model predicting earnings from height. What transformation should you perform in order to interpret the intercept from this model as average earnings for people with average height?

```{r}
earn <- heights.complete$earn
sumheight <- heights.complete$height1 + heights.complete$height2
weight <- heights.complete$height
```
**In order to interpret the intercept as average earning for people with average height, we should make a center transformation to the linear regression using earnings subtracting the mean of earnings.**
```{r}
center.height <- sumheight - mean(sumheight)
regout.1 <- lm(earn ~ center.height)
summary(regout.1)
```
**The formula is \[earn = 607.6height_{centered} + 23154.8\] a person with average height is supposed to have the earning of 23154.8.**

3. Fit some regression models with the goal of predicting earnings from some
combination of sex, height, and weight. Be sure to try various transformations and interactions that might make sense. Choose your preferred model and justify.

```{r}
#Regress earnings on centered height and centered weight
center.weight <- weight - mean(weight)
regout.2 <- lm(earn ~ center.height + center.weight, heights.complete)
summary(regout.2)
#Plot the residuals
plot(resid(regout.2))
abline(h = 0)
```
**The coefficients are large so we make a log transformation to the linear regression.**
```{r}
#Make a log transformation on the earn
regout.3 <- lm(log(earn) ~ center.height + center.weight, heights.complete)
sumary(regout.3)
#Plot the residuals
plot(resid(regout.3))
abline(h = 0)
```
```{r}
#Transform earnings by dividing it by its mean
regout.4 <- lm((earn/mean(earn)) ~ center.height + center.weight, heights.complete)
summary(regout.4)
plot(resid(regout.4))
abline(h = 0)
```
**Upon the transformation above, I prefer the regout.3. It makes a log transformation for earn to make a linear regression on centered height and centered weight. The results have the proper coefficients and the residuals are small.**

4. Interpret all model coefficients.
As regression 1 for example, for regressing earnings on centered height in regout.1:
When the height is the average value, the earning is supposed to be 23154.8.
After the height increases 1 unit, the earning increases 607.6.

5. Construct 95% confidence interval for all model coefficients and discuss what they mean.

```{r}
confint(regout.1, level = 0.95)
```
* As regressing earnings on centered height for example:
We have the confidence of 95% that the range [22052.83,24256.71] includes the true value of intercept of the regression. It is not acrossing the 0, so it is statistically significant. Similarly, We have the confidence of 95% that the range [247.76,967,46] includes the true value of centered height's true value. It is not acrossing the 0, so it is statistically significant. 


### Analysis of mortality rates and various environmental factors

The folder `pollution` contains mortality rates and various environmental factors from 60 U.S. metropolitan areas from McDonald, G.C. and Schwing, R.C. (1973) 'Instabilities of regression estimates relating air pollution to mortality', Technometrics, vol.15, 463-482. 

Variables, in order:

* PREC   Average annual precipitation in inches
* JANT   Average January temperature in degrees F
* JULT   Same for July
* OVR65  % of 1960 SMSA population aged 65 or older
* POPN   Average household size
* EDUC   Median school years completed by those over 22
* HOUS   % of housing units which are sound & with all facilities
* DENS   Population per sq. mile in urbanized areas, 1960
* NONW   % non-white population in urbanized areas, 1960
* WWDRK  % employed in white collar occupations
* POOR   % of families with income < $3000
* HC     Relative hydrocarbon pollution potential
* NOX    Same for nitric oxides
* SO2    Same for sulphur dioxide
* HUMID  Annual average % relative humidity at 1pm
* MORT   Total age-adjusted mortality rate per 100,000

For this exercise we shall model mortality rate given nitric oxides, sulfur dioxide, and hydrocarbons as inputs. This model is an extreme oversimplification as it combines all sources of mortality and does not adjust for crucial factors such as age and smoking. We use it to illustrate log transformations in regression.

```{r}
gelman_dir   <- "http://www.stat.columbia.edu/~gelman/arm/examples/"
pollution    <- read.dta (paste0(gelman_dir,"pollution/pollution.dta"))
```

1. Create a scatterplot of mortality rate versus level of nitric oxides. Do you think linear regression will fit these data well? Fit the regression and evaluate a residual plot from the regression.

```{r}
#Make the scatterplot
library("ggplot2")
ggplot(data = pollution, aes(x = nox, y = mort)) + 
	geom_point()

```

```{r}
(regout.5 <- lm(mort ~ nox, pollution))
#Make the plot
ggplot(pollution, aes(x <- nox, y <- mort)) +
  geom_point() +
  stat_smooth(method = lm, col = "red")
par(mfrow = c(2, 2))  # Split the plotting panel into a 2 x 2 grid
plot(regout.5) # Plot the residuals
```
It seems that it does not fit the linear regression model well. The residuals seems not good for the model. Maybe we should make some transformation for the variables of the model.

2. Find an appropriate transformation that will result in data more appropriate for linear regression. Fit a regression to the transformed data and evaluate the new residual plot.
**We can find that the points are gathered in the left of the scatter plot, so we can use log(nox) to fit the model.**

```{r}
#Regress mort on log(nox)
regout.6 <- lm(mort ~ log(nox), pollution)
summary(regout.6)
#Make the new plot
ggplot(pollution, aes(x <- log(nox), y <- mort)) +
  geom_point() +
  stat_smooth(method = lm, col = "red")
par(mfrow = c(2, 2))  # Split the plotting panel into a 2 x 2 grid
plot(regout.6) # Plot

```
**It seems that the regression has better fit for the variables and the residuals versus fitted value looks better than the formal regression.**

3. Interpret the slope coefficient from the model you chose in 2.
The fomula in 2 is \[mort = 15log(nox) + 905\].
Controlling other variables, 1 unit change in \[log(nox)\] will lead to 15 unit change in mortality.


4. Construct 99% confidence interval for slope coefficient from the model you chose in 2 and interpret them.

```{r}
confint(regout.6, level = 0.99)
```
We have the confidence of 99% that the range [-2, 33] includes the true value of the slope coeffiecient of the regression. It is acrossing the 0, so we cannot say it is statistically significant.

5. Now fit a model predicting mortality rate using levels of nitric oxides, sulfur dioxide, and hydrocarbons as inputs. Use appropriate transformations when helpful. Plot the fitted regression model and interpret the coefficients.

**Similarly to above analysis, we should make a log transformation to the right of the equation. As follows:**
```{r}
#Make a linear regression
regout.7 = lm(mort ~ log(nox) + log(so2) + log(hc), data = pollution)
summary(regout.7)
#Plot it
ggplot(pollution, aes(x <- log(nox) + log(so2) + log(hc), y <- mort)) +
  geom_point() +
  stat_smooth(method = lm, color = "red")
```
**When the other variables are all 0, the total age-adjusted mortality rate per 100,000 is 925. When $log(nox)$ increases 1 unit, the mortality rate increases 58; when $log(so2)$ increases 1 unit, the mortality rate increases 12; when $log(hc)$ increases 1 unit, the mortality rate decreases 57. **

6. Cross-validate: fit the model you chose above to the first half of the data and then predict for the second half. (You used all the data to construct the model in 4, so this is not really cross-validation, but it gives a sense of how the steps of cross-validation can be implemented.)

```{r}
#Fit the model
dim(pollution)
pollution.fit <- pollution[1:30,]
pollution.pred <- pollution[31:60,]
regout.8 <- lm(mort ~ log(nox) + log(so2) + log(hc), data = pollution.fit)
summary(regout.8)

#Make prediction
pred <- predict(object = regout.8, newdata = data.frame(mort = pollution.pred$mort, 
                                               nox = pollution.pred$nox, 
                                               so2 = pollution.pred$so2, 
                                               hc = pollution.pred$hc), interval = "prediction")
#the difference between the actual values and the predicted values
difference <- pred[,1] - pollution[31:60,]$mort
plot(difference)
abline(h = 0)
```

### Study of teenage gambling in Britain

```{r,message =FALSE}
data(teengamb)
?teengamb
```

1. Fit a linear regression model with gamble as the response and the other variables as predictors and interpret the coefficients. Make sure you rename and transform the variables to improve the interpretability of your regression model.

```{r}
teengamb <- teengamb
status.center <- teengamb$status - mean(teengamb$status)
verbal.center <- teengamb$verbal - mean(teengamb$verbal)
regout.8 <- lm(log(gamble+1) ~ sex + status.center + log(income) + verbal.center, data = teengamb)
summary(regout.8)

```
**The fomula is as follows:**
\[log(gamble+1)=1.22-1.10sex+0.02status.center+0.93log(income)-0.25verbal.center\]

2. Create a 95% confidence interval for each of the estimated coefficients and discuss how you would interpret this uncertainty.

```{r}
confint(regout.8, level = 0.95)
```
We have the confidence of 95% that the range [0.41,2.02] will include the intercept's true value; the range doesn't cross 0 so the intercept is significant. Similarly, the range [-1.89,-0.31] will include the sex coefficient's true value; the range [-0.003,0.05] will include the centered status coefficient's true value; the range [0.46,1.41] will include the log income coefficient' true value; the range [-0.47,-0.039] will include the centered verbal coefficient's true value; the range doesn't cross zero so the coefficient is significant.

3. Predict the amount that a male with average status, income and verbal score would gamble along with an appropriate 95% CI.  Repeat the prediction for a male with maximal values of status, income and verbal score.  Which CI is wider and why is this result expected?

```{r}
prediction.average <- predict(object = regout.8, newdata = data.frame(sex=0, status.center = mean(teengamb$status) - mean(teengamb$status), income = log(mean(teengamb$income)), verbal.center = mean(teengamb$verbal) - mean(teengamb$verbal)), level = 0.95,interval  = "prediction")
prediction.average

prediction.max <- predict(object = regout.8,newdata = data.frame(sex = 0, status.center = max(teengamb$status) - mean(teengamb$status), income = log(max(teengamb$income)), verbal.center = max(teengamb$verbal) - mean(teengamb$verbal)), level = 0.95,interval = "prediction")
prediction.max
```

### School expenditure and test scores from USA in 1994-95

```{r}
data(sat)
?sat
```

1. Fit a model with total sat score as the outcome and expend, ratio and salary as predictors.  Make necessary transformation in order to improve the interpretability of the model.  Interpret each of the coefficient.

```{r}
#Regress total sat score on expend, rario and aslary
regout.9 <- lm(total ~ expend + ratio + salary, data = sat)
summary(regout.9)

#Make z-score transformation
ratio.center = sat$ratio - mean(sat$ratio)
z.expend = (sat$expend - mean(sat$expend)) / 2*sd(sat$expend)
z.salary = (sat$salary -mean(sat$salary))/2*sd(sat$salary)
regout.10 <- lm(total ~ z.expend + ratio.center + z.salary, data = sat)
summary(regout.10)

#Make log transformation on expend, ratio and salary
regout.11 <- lm(total ~ log(expend) + log(ratio) + log(salary), data = sat)
summary(regout.11)

#Make log tranformation on total sat score
regout.12 <- lm(log(total) ~ expend + ratio + salary, data = sat)
summary(regout.12)
```
The z-score transformation seems the best among the four types of transformation.

2. Construct 98% CI for each coefficient and discuss what you see.

```{r}
confint(regout.10, level = 0.98)
```
Only the intercept does not across 0 and is statistically significant. Others all across 0 and are not statistically significiant.

3. Now add takers to the model.  Compare the fitted model to the previous model and discuss which of the model seem to explain the outcome better?

```{r}
#Regress total sat score on expend, rario, aslary and takers
regout.13 <- lm(total ~ expend + ratio + salary + takers, data = sat)
summary(regout.13)

#Make z-score transformation
takers.center <- sat$takers - mean(sat$takers)
regout.14 <- lm(total ~ z.expend + ratio.center + z.salary + takers.center, data = sat)
summary(regout.14)

#Make log transformation on expend, ratio, salary and takers
regout.15 <- lm(total ~ log(expend) + log(ratio) + log(salary) + log(takers), data = sat)
summary(regout.15)

#Make log tranformation on total sat score
regout.16 <- lm(log(total) ~ expend + ratio + salary + takers, data = sat)
summary(regout.16)
```
Compared with the formal regressions, the regressions in 3 are better for the better r square. 

# Conceptual exercises.

### Special-purpose transformations:

For a study of congressional elections, you would like a measure of the relative amount of money raised by each of the two major-party candidates in each district. Suppose that you know the amount of money raised by each candidate; label these dollar values $D_i$ and $R_i$. You would like to combine these into a single variable that can be included as an input variable into a model predicting vote share for the Democrats.

Discuss the advantages and disadvantages of the following measures:

* The simple difference, $D_i-R_i$

It is the numeric difference beteen two variables. It can be input to compare the vote share based on the numeric difference of the amount of money raised. But it ignores the proportion.

* The ratio, $D_i/R_i$

Using this variable, we can compare when one party has the 1 percent more money raised how much the party has vote share more than the other one. But it ignores the actual number of each party.

* The difference on the logarithmic scale, $log D_i-log R_i$ 

$log D_i-log R_i = log (D_i/R_i)$, so it can reflect the proportion of the relation. In some case, it can simplify the problem. But it ignores the actual number too.

* The relative proportion, $D_i/(D_i+R_i)$.

It uses the whole money raised for the two parties, takes the whole volumn into account to compare the difference of the money raised related the vote share. However, it can be hard to interpret.

### Transformation 

For observed pair of $\mathrm{x}$ and $\mathrm{y}$, we fit a simple regression model 
$$\mathrm{y}=\alpha + \beta \mathrm{x} + \mathrm{\epsilon}$$ 
which results in estimates $\hat{\alpha}=1$, $\hat{\beta}=0.9$, $SE(\hat{\beta})=0.03$, $\hat{\sigma}=2$ and $r=0.3$.

1. Suppose that the explanatory variable values in a regression are transformed according to the $\mathrm{x}^{\star}=\mathrm{x}-10$ and that $\mathrm{y}$ is regressed on $\mathrm{x}^{\star}$.  Without redoing the regression calculation in detail, find $\hat{\alpha}^{\star}$, $\hat{\beta}^{\star}$, $\hat{\sigma}^{\star}$, and $r^{\star}$.  What happens to these quantities when $\mathrm{x}^{\star}=10\mathrm{x}$ ? When $\mathrm{x}^{\star}=10(\mathrm{x}-1)$?


2. Now suppose that the response variable scores are transformed according to the formula
$\mathrm{y}^{\star\star}= \mathrm{y}+10$ and that $\mathrm{y}^{\star\star}$ is regressed on $\mathrm{x}$.  Without redoing the regression calculation in detail, find $\hat{\alpha}^{\star\star}$, $\hat{\beta}^{\star\star}$, $\hat{\sigma}^{\star\star}$, and $r^{\star\star}$.  What happens to these quantities when $\mathrm{y}^{\star\star}=5\mathrm{y}$ ? When $\mathrm{y}^{\star\star}=5(\mathrm{y}+2)$?


3. In general, how are the results of a simple regression analysis affected by linear transformations of $\mathrm{y}$ and $\mathrm{x}$?



4. Suppose that the explanatory variable values in a regression are transformed according to the $\mathrm{x}^{\star}=10(\mathrm{x}-1)$ and that $\mathrm{y}$ is regressed on $\mathrm{x}^{\star}$.  Without redoing the regression calculation in detail, find $SE(\hat{\beta}^{\star})$ and $t^{\star}_0= \hat{\beta}^{\star}/SE(\hat{\beta}^{\star})$.


5. Now suppose that the response variable scores are transformed according to the formula
$\mathrm{y}^{\star\star}=5(\mathrm{y}+2)$ and that $\mathrm{y}^{\star\star}$ is regressed on $\mathrm{x}$.  Without redoing the regression calculation in detail, find $SE(\hat{\beta}^{\star\star})$ and $t^{\star\star}_0= \hat{\beta}^{\star\star}/SE(\hat{\beta}^{\star\star})$.


6. In general, how are the hypothesis tests and confidence intervals for $\beta$ affected by linear transformations of $\mathrm{y}$ and $\mathrm{x}$?



		
# Feedback comments etc.

If you have any comments about the homework, or the class, please write your feedback here.  We love to hear your opinions.

